---
layout: page
title: "Attention Is All You Need"
permalink: /llm/algorithm/transformer-paper/
description: "Transformer架构的经典论文"
---

# Attention Is All You Need

## 论文信息

- **标题**: Attention Is All You Need
- **作者**: Vaswani et al.
- **年份**: 2017
- **领域**: 模型架构
- **关键词**: Transformer, Attention Mechanism, Neural Machine Translation

## 摘要

本文提出了Transformer架构，这是第一个完全基于注意力机制的序列到序列模型...

## 主要贡献

1. **自注意力机制**: 提出了多头自注意力机制
2. **位置编码**: 使用正弦和余弦函数进行位置编码
3. **架构设计**: 编码器-解码器结构，每层包含自注意力和前馈网络

## 技术细节

### 自注意力机制

自注意力机制允许模型关注输入序列的不同部分...

### 多头注意力

多头注意力机制将查询、键、值投影到不同的子空间...

## 实验结果

在WMT 2014英德翻译任务上，Transformer模型达到了28.4 BLEU分数...

## 影响与意义

Transformer架构为后续的大语言模型奠定了基础...

## 相关论文

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](/path/to/bert-paper/)
- [GPT: Improving Language Understanding by Generative Pre-Training](/path/to/gpt-paper/)

---

*返回 [研究论文](/blog/)* 